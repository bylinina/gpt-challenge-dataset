{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b3b4a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "744d6a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d0e7138",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"./results/multinli_transforms_bertscore_dev_matched.jsonl\", lines=True)\n",
    "df = df[df['gold_label'] != '-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08339026",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sentence_pairs = [(row['sentence1'], row['sentence2']) for index, row in list(df.iterrows())]\n",
    "sentence_pairs_transformed = [\n",
    "    [row['sentence1_transform1'] if row['sentence1_transform1_bertscore'] > row['sentence1_transform2_bertscore'] else row['sentence1_transform2'],\n",
    "     row['sentence2_transform1'] if row['sentence2_transform1_bertscore'] > row['sentence2_transform2_bertscore'] else row['sentence2_transform2']] \n",
    "    for index, row in list(df.iterrows())\n",
    "]\n",
    "\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c203069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erwin/.virtualenvs/thesis/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n",
      "3800\n",
      "3850\n",
      "3900\n",
      "3950\n",
      "4000\n",
      "4050\n",
      "4100\n",
      "4150\n",
      "4200\n",
      "4250\n",
      "4300\n",
      "4350\n",
      "4400\n",
      "4450\n",
      "4500\n",
      "4550\n",
      "4600\n",
      "4650\n",
      "4700\n",
      "4750\n",
      "4800\n",
      "4850\n",
      "4900\n",
      "4950\n",
      "5000\n",
      "5050\n",
      "5100\n",
      "5150\n",
      "5200\n",
      "5250\n",
      "5300\n",
      "5350\n",
      "5400\n",
      "5450\n",
      "5500\n",
      "5550\n",
      "5600\n",
      "5650\n",
      "5700\n",
      "5750\n",
      "5800\n",
      "5850\n",
      "5900\n",
      "5950\n",
      "6000\n",
      "6050\n",
      "6100\n",
      "6150\n",
      "6200\n",
      "6250\n",
      "6300\n",
      "6350\n",
      "6400\n",
      "6450\n",
      "6500\n",
      "6550\n",
      "6600\n",
      "6650\n",
      "6700\n",
      "6750\n",
      "6800\n",
      "6850\n",
      "6900\n",
      "6950\n",
      "7000\n",
      "7050\n",
      "7100\n",
      "7150\n",
      "7200\n",
      "7250\n",
      "7300\n",
      "7350\n",
      "7400\n",
      "7450\n",
      "7500\n",
      "7550\n",
      "7600\n",
      "7650\n",
      "7700\n",
      "7750\n",
      "7800\n",
      "7850\n",
      "7900\n",
      "7950\n",
      "8000\n",
      "8050\n",
      "8100\n",
      "8150\n",
      "8200\n",
      "8250\n",
      "8300\n",
      "8350\n",
      "8400\n",
      "8450\n",
      "8500\n",
      "8550\n",
      "8600\n",
      "8650\n",
      "8700\n",
      "8750\n",
      "8800\n",
      "8850\n",
      "8900\n",
      "8950\n",
      "9000\n",
      "9050\n",
      "9100\n",
      "9150\n",
      "9200\n",
      "9250\n",
      "9300\n",
      "9350\n",
      "9400\n",
      "9450\n",
      "9500\n",
      "9550\n",
      "9600\n",
      "9650\n",
      "9700\n",
      "9750\n",
      "9800\n"
     ]
    }
   ],
   "source": [
    "# pose sequence as a NLI premise and label as a hypothesis\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained('cross-encoder/nli-deberta-v3-large').to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained('cross-encoder/nli-deberta-v3-large')\n",
    "\n",
    "# batch to prevent memory issues\n",
    "for i in range(0, len(sentence_pairs), 50):\n",
    "    print(i)\n",
    "    sentence_pair_batch = sentence_pairs_transformed[i:i+50]\n",
    "\n",
    "    # run through model pre-trained on MNLI\n",
    "    features = tokenizer(sentence_pair_batch, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "    nli_model.eval()\n",
    "    with torch.no_grad():\n",
    "        scores = nli_model(**features).logits\n",
    "        label_mapping = ['contradiction', 'entailment', 'neutral']\n",
    "        labels.extend([label_mapping[score_max] for score_max in scores.argmax(dim=1)])\n",
    "    \n",
    "    del features\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4131e88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9815\n"
     ]
    }
   ],
   "source": [
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfa91dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_labels = df['gold_label'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c454325",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = sum([prediction == gold_label for prediction, gold_label in zip(labels, gold_labels)]) / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b2d14b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7833927661742232\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba9419a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['deberta_large_transform_label'] = labels\n",
    "new_df = df[['pairID', 'deberta_large_transform_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f10eeff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9815, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78e00f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(\"./results/deberta_large_transform_labels.jsonl\")\n",
    "new_df.to_json(save_path, orient=\"records\", lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
